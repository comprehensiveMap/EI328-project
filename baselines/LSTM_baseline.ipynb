{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ac82ed2970>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(54749110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = []\n",
    "train_label_list = []\n",
    "test_data_list = []\n",
    "test_label_list = []\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = MaxAbsScaler()\n",
    "\n",
    "for i in range(1, 11):\n",
    "    mat_data = loadmat(\"data/train/\"+str(i)+\".mat\")\n",
    "    train_data_list.append(scaler.fit_transform(mat_data['de_feature']))\n",
    "    train_label_list.append(mat_data['label'])\n",
    "\n",
    "for i in range(11, 14):\n",
    "    mat_data = loadmat(\"data/test/\"+str(i)+\".mat\")\n",
    "    test_data_list.append(scaler.fit_transform(mat_data['de_feature']))\n",
    "    test_label_list.append(mat_data['label'])\n",
    "\n",
    "train_datas = np.concatenate(train_data_list)\n",
    "train_labels = np.concatenate(train_label_list)\n",
    "test_datas = np.concatenate(test_data_list)\n",
    "test_labels = np.concatenate(test_label_list)\n",
    "\n",
    "# pca = PCA(n_components=10)\n",
    "# train_datas = pca.fit_transform(train_datas)\n",
    "# test_data_list = [pca.fit_transform(x) for x in test_data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentimentDataset(Dataset):\n",
    "    def __init__(self, data, labels=None, window_size=6):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.len = data.shape[0]//window_size\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.window_size\n",
    "        data_tensor = torch.tensor(self.data[start: start+self.window_size], dtype=torch.float32)\n",
    "        if self.labels is not None:\n",
    "            a = self.labels[start: start+self.window_size]\n",
    "            label_tensor = torch.tensor(max(a.tolist(), key=a.tolist().count), dtype=torch.long)\n",
    "        return data_tensor, label_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testsets = [sentimentDataset(test_data_list[i], test_label_list[i]) for i in range(3)]\n",
    "# testloaders = [DataLoader(testset, batch_size=64) for testset in testsets]\n",
    "# it = iter(testloaders[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "DEV_NUM = 0\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "class LSTM_baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(310, HIDDEN_SIZE, batch_first=True)\n",
    "        self.fc = nn.Sequential(nn.Linear(512, 128),\n",
    "                               nn.ReLU(True),\n",
    "                               nn.Linear(128, 4))\n",
    "    \n",
    "    def forward(self, datas, labels=None): # datas: [batch_size, window_size, feature_dim(310)]\n",
    "#         print(datas.shape)\n",
    "        output, (hn, cn) = self.lstm(datas) # hn: [batch_size, hidden_size]\n",
    "        output = output.view(-1, datas.shape[1], HIDDEN_SIZE)\n",
    "        logits = self.fc(hn)\n",
    "        logit = F.softmax(logits)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            loss_fnt = nn.CrossEntropyLoss()\n",
    "            loss = loss_fnt(logits.view(-1, 4), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for sample in dataloader:\n",
    "            datas = sample[0]\n",
    "            if torch.cuda.is_available():\n",
    "                datas = datas.to(\"cuda\")\n",
    "            outputs = model(datas)\n",
    "            logits = F.softmax(outputs[0].squeeze(), dim=1)\n",
    "            _, pred = torch.max(logits.data, dim=1)\n",
    "            if compute_acc:\n",
    "                labels = sample[1]\n",
    "                if torch.cuda.is_available():\n",
    "                    labels = labels.to(\"cuda\")\n",
    "                total += labels.shape[0]\n",
    "                correct += (pred == labels.squeeze()).sum().item()\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "\n",
    "    model.train()\n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    else:\n",
    "        return predictions\n",
    "        \n",
    "\n",
    "def train_model(model, trainset, validloaders: list):\n",
    "    trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "    device = torch.device(\"cuda:\"+str(DEV_NUM) if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 5e-5)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for datas, labels in trainloader:\n",
    "            datas = datas.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(datas, labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        _, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "        pres_and_accs = [get_predictions(model, validloader, compute_acc=True) for validloader in validloaders]\n",
    "        accs = np.array([x[1] for x in pres_and_accs])\n",
    "        print(\"In epoch %d, running_loss: %.3f, train_acc: %.3f, valid_avg_acc: %.3f,\" %(epoch, running_loss, train_acc, accs.mean())\\\n",
    "             + \" accs: \" + str(accs))\n",
    "    print(\"Training done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = sentimentDataset(train_datas, train_labels)\n",
    "trainloader = DataLoader(trainset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, running_loss: 31.673, train_acc: 0.307, valid_avg_acc: 0.314, accs: [0.30496454 0.30496454 0.33333333]\n",
      "In epoch 1, running_loss: 31.280, train_acc: 0.330, valid_avg_acc: 0.314, accs: [0.30496454 0.30496454 0.33333333]\n",
      "In epoch 2, running_loss: 30.646, train_acc: 0.453, valid_avg_acc: 0.319, accs: [0.27659574 0.29787234 0.38297872]\n",
      "In epoch 3, running_loss: 29.572, train_acc: 0.511, valid_avg_acc: 0.296, accs: [0.29787234 0.31914894 0.26950355]\n",
      "In epoch 4, running_loss: 27.433, train_acc: 0.530, valid_avg_acc: 0.303, accs: [0.34042553 0.23404255 0.33333333]\n",
      "In epoch 5, running_loss: 24.798, train_acc: 0.648, valid_avg_acc: 0.310, accs: [0.43262411 0.22695035 0.26950355]\n",
      "In epoch 6, running_loss: 21.875, train_acc: 0.721, valid_avg_acc: 0.388, accs: [0.43971631 0.34751773 0.37588652]\n",
      "In epoch 7, running_loss: 18.895, train_acc: 0.740, valid_avg_acc: 0.456, accs: [0.43262411 0.4822695  0.45390071]\n",
      "In epoch 8, running_loss: 17.495, train_acc: 0.794, valid_avg_acc: 0.468, accs: [0.42553191 0.5106383  0.46808511]\n",
      "In epoch 9, running_loss: 14.882, train_acc: 0.774, valid_avg_acc: 0.452, accs: [0.43971631 0.4751773  0.43971631]\n",
      "In epoch 10, running_loss: 13.980, train_acc: 0.812, valid_avg_acc: 0.506, accs: [0.41134752 0.56028369 0.54609929]\n",
      "In epoch 11, running_loss: 12.836, train_acc: 0.826, valid_avg_acc: 0.468, accs: [0.45390071 0.5106383  0.43971631]\n",
      "In epoch 12, running_loss: 11.804, train_acc: 0.858, valid_avg_acc: 0.473, accs: [0.33333333 0.57446809 0.5106383 ]\n",
      "In epoch 13, running_loss: 10.980, train_acc: 0.882, valid_avg_acc: 0.466, accs: [0.38297872 0.54609929 0.46808511]\n",
      "In epoch 14, running_loss: 9.809, train_acc: 0.877, valid_avg_acc: 0.470, accs: [0.43262411 0.5177305  0.46099291]\n",
      "In epoch 15, running_loss: 9.661, train_acc: 0.882, valid_avg_acc: 0.511, accs: [0.42553191 0.54609929 0.56028369]\n",
      "In epoch 16, running_loss: 8.714, train_acc: 0.885, valid_avg_acc: 0.437, accs: [0.34751773 0.53191489 0.43262411]\n",
      "In epoch 17, running_loss: 8.141, train_acc: 0.922, valid_avg_acc: 0.482, accs: [0.36879433 0.56028369 0.5177305 ]\n",
      "In epoch 18, running_loss: 7.403, train_acc: 0.924, valid_avg_acc: 0.475, accs: [0.39007092 0.54609929 0.4893617 ]\n",
      "In epoch 19, running_loss: 6.698, train_acc: 0.929, valid_avg_acc: 0.518, accs: [0.40425532 0.58156028 0.56737589]\n",
      "In epoch 20, running_loss: 6.956, train_acc: 0.934, valid_avg_acc: 0.508, accs: [0.43262411 0.56028369 0.53191489]\n",
      "In epoch 21, running_loss: 6.155, train_acc: 0.940, valid_avg_acc: 0.515, accs: [0.41843972 0.58156028 0.54609929]\n",
      "In epoch 22, running_loss: 5.876, train_acc: 0.938, valid_avg_acc: 0.444, accs: [0.36879433 0.5106383  0.45390071]\n",
      "In epoch 23, running_loss: 5.640, train_acc: 0.947, valid_avg_acc: 0.454, accs: [0.31205674 0.56737589 0.4822695 ]\n",
      "In epoch 24, running_loss: 5.411, train_acc: 0.954, valid_avg_acc: 0.506, accs: [0.38297872 0.58865248 0.54609929]\n",
      "Training done...\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_baseline()\n",
    "trainset = sentimentDataset(train_datas, train_labels)\n",
    "testsets = [sentimentDataset(test_data_list[i], test_label_list[i]) for i in range(3)]\n",
    "testloaders = [DataLoader(testset, batch_size=64) for testset in testsets]\n",
    "train_model(model, trainset, testloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_gpu] *",
   "language": "python",
   "name": "conda-env-tensorflow_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
