{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1a1ca532970>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(31415926)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = []\n",
    "train_label_list = []\n",
    "test_data_list = []\n",
    "test_label_list = []\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = MaxAbsScaler()\n",
    "\n",
    "for i in range(1, 11):\n",
    "    mat_data = loadmat(\"data/train/\"+str(i)+\".mat\")\n",
    "    train_data_list.append(scaler.fit_transform(mat_data['de_feature']))\n",
    "    train_label_list.append(mat_data['label'])\n",
    "\n",
    "for i in range(11, 14):\n",
    "    mat_data = loadmat(\"data/test/\"+str(i)+\".mat\")\n",
    "    test_data_list.append(scaler.fit_transform(mat_data['de_feature']))\n",
    "    test_label_list.append(mat_data['label'])\n",
    "\n",
    "train_datas = np.concatenate(train_data_list)\n",
    "train_labels = np.concatenate(train_label_list)\n",
    "test_datas = np.concatenate(test_data_list)\n",
    "test_labels = np.concatenate(test_label_list)\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# train_datas = pca.fit_transform(train_datas)\n",
    "# test_data_list = [pca.fit_transform(x) for x in test_data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(851, 310)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_list[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentimentDataset(Dataset):\n",
    "    def __init__(self, data, labels=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.len = data.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data_tensor = torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "        if self.labels is not None:\n",
    "            label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return data_tensor, label_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "DEV_NUM = 0\n",
    "IN_FEATURE_DIM = 310\n",
    "\n",
    "class baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(IN_FEATURE_DIM, 256),\n",
    "                               nn.ReLU(True),\n",
    "                               nn.Linear(256, 64),\n",
    "                               nn.ReLU(True),\n",
    "                               nn.Linear(64, 4))\n",
    "    \n",
    "    def forward(self, datas, labels=None):\n",
    "        logits = self.fc(datas)\n",
    "#         logit = F.softmax(logits)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            loss_fnt = nn.CrossEntropyLoss()\n",
    "            loss = loss_fnt(logits.view(-1, 4), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "class cnn_baseline(nn.Module):\n",
    "    def __init__(self, out_channels=256):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, out_channels, kernel_size=[1, 62])\n",
    "        self.fc1 = nn.Sequential(nn.Linear(out_channels, 64),\n",
    "                               nn.ReLU(True),\n",
    "                               nn.Linear(64, 4))\n",
    "        \n",
    "    def forward(self, datas, labels=None):\n",
    "        datas = datas.view(-1, 5, 62) # (batch_size, 5, 62)\n",
    "        expand_datas = datas.unsqueeze(1) # (batch_size, 1, 5, 62)\n",
    "        conved = self.conv1(expand_datas).squeeze() # (batch_size, out_channels, 5)\n",
    "        pooled = F.max_pool1d(conved, kernel_size=5).squeeze() # (batch_size, out_channels)\n",
    "        logits = self.fc1(pooled)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            loss_fnt = nn.CrossEntropyLoss()\n",
    "            loss = loss_fnt(logits.view(-1, 4), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for sample in dataloader:\n",
    "            datas = sample[0]\n",
    "            if torch.cuda.is_available():\n",
    "                datas = datas.to(\"cuda\")\n",
    "            outputs = model(datas)\n",
    "            logits = F.softmax(outputs[0], dim=1)\n",
    "            _, pred = torch.max(logits.data, dim=1)\n",
    "            if compute_acc:\n",
    "                labels = sample[1]\n",
    "                if torch.cuda.is_available():\n",
    "                    labels = labels.to(\"cuda\")\n",
    "                total += labels.shape[0]\n",
    "                correct += (pred == labels.squeeze()).sum().item()\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "\n",
    "    model.train()\n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    else:\n",
    "        return predictions\n",
    "        \n",
    "\n",
    "def train_model(model, trainset, validloaders: list):\n",
    "    trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "    device = torch.device(\"cuda:\"+str(DEV_NUM) if torch.cuda.is_available() else \"cpu\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for datas, labels in trainloader:\n",
    "            datas = datas.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(datas, labels)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        _, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "        pres_and_accs = [get_predictions(model, validloader, compute_acc=True) for validloader in validloaders]\n",
    "        accs = np.array([x[1] for x in pres_and_accs])\n",
    "        print(\"In epoch %d, running_loss: %.3f, train_acc: %.3f, valid_avg_acc: %.3f,\" %(epoch, running_loss, train_acc, accs.mean())\\\n",
    "             + \" accs: \" + str(accs))\n",
    "    print(\"Training done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, running_loss: 175.389, train_acc: 0.527, valid_avg_acc: 0.330, accs: [0.33842538 0.30904818 0.34312573]\n",
      "In epoch 1, running_loss: 152.133, train_acc: 0.628, valid_avg_acc: 0.373, accs: [0.29847239 0.47943596 0.33960047]\n",
      "In epoch 2, running_loss: 128.687, train_acc: 0.694, valid_avg_acc: 0.379, accs: [0.29377203 0.50293772 0.33960047]\n",
      "In epoch 3, running_loss: 109.636, train_acc: 0.766, valid_avg_acc: 0.503, accs: [0.41363102 0.57579318 0.51938895]\n",
      "In epoch 4, running_loss: 93.520, train_acc: 0.796, valid_avg_acc: 0.523, accs: [0.42185664 0.613396   0.53349001]\n",
      "In epoch 5, running_loss: 80.582, train_acc: 0.825, valid_avg_acc: 0.513, accs: [0.41715629 0.5840188  0.53701528]\n",
      "In epoch 6, running_loss: 69.812, train_acc: 0.869, valid_avg_acc: 0.476, accs: [0.39365452 0.54524089 0.48766157]\n",
      "In epoch 7, running_loss: 60.800, train_acc: 0.898, valid_avg_acc: 0.498, accs: [0.42068155 0.56874266 0.5052879 ]\n",
      "In epoch 8, running_loss: 52.790, train_acc: 0.921, valid_avg_acc: 0.509, accs: [0.43830787 0.56874266 0.52056404]\n",
      "In epoch 9, running_loss: 45.947, train_acc: 0.935, valid_avg_acc: 0.513, accs: [0.44065805 0.60869565 0.49001175]\n",
      "In epoch 10, running_loss: 39.540, train_acc: 0.958, valid_avg_acc: 0.483, accs: [0.386604   0.5840188  0.47943596]\n",
      "In epoch 11, running_loss: 34.155, train_acc: 0.966, valid_avg_acc: 0.497, accs: [0.37837838 0.64512338 0.46886016]\n",
      "In epoch 12, running_loss: 29.467, train_acc: 0.976, valid_avg_acc: 0.506, accs: [0.38425382 0.65569918 0.47943596]\n",
      "In epoch 13, running_loss: 25.120, train_acc: 0.985, valid_avg_acc: 0.512, accs: [0.38190364 0.63572268 0.51703878]\n",
      "In epoch 14, running_loss: 21.579, train_acc: 0.981, valid_avg_acc: 0.484, accs: [0.39247944 0.59811986 0.46180964]\n",
      "In epoch 15, running_loss: 18.687, train_acc: 0.985, valid_avg_acc: 0.512, accs: [0.40423032 0.62044653 0.51116334]\n",
      "In epoch 16, running_loss: 16.166, train_acc: 0.993, valid_avg_acc: 0.488, accs: [0.35957697 0.62632197 0.47826087]\n",
      "In epoch 17, running_loss: 13.841, train_acc: 0.997, valid_avg_acc: 0.492, accs: [0.36192714 0.65569918 0.45710928]\n",
      "In epoch 18, running_loss: 12.119, train_acc: 0.997, valid_avg_acc: 0.494, accs: [0.37250294 0.64982374 0.45945946]\n",
      "In epoch 19, running_loss: 10.473, train_acc: 0.998, valid_avg_acc: 0.489, accs: [0.36310223 0.65687427 0.44653349]\n",
      "In epoch 20, running_loss: 8.930, train_acc: 0.998, valid_avg_acc: 0.474, accs: [0.35840188 0.61809636 0.4453584 ]\n",
      "In epoch 21, running_loss: 7.772, train_acc: 1.000, valid_avg_acc: 0.481, accs: [0.34312573 0.65452409 0.44653349]\n",
      "In epoch 22, running_loss: 6.899, train_acc: 1.000, valid_avg_acc: 0.486, accs: [0.35487662 0.65569918 0.44888367]\n",
      "In epoch 23, running_loss: 5.926, train_acc: 0.999, valid_avg_acc: 0.482, accs: [0.35017626 0.63689777 0.45828437]\n",
      "In epoch 24, running_loss: 5.182, train_acc: 1.000, valid_avg_acc: 0.488, accs: [0.35722679 0.65804935 0.44888367]\n",
      "Training done...\n"
     ]
    }
   ],
   "source": [
    "model = baseline()\n",
    "trainset = sentimentDataset(train_datas, train_labels)\n",
    "testsets = [sentimentDataset(test_data_list[i], test_label_list[i]) for i in range(3)]\n",
    "testloaders = [DataLoader(testset, batch_size=64) for testset in testsets]\n",
    "train_model(model, trainset, testloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_gpu] *",
   "language": "python",
   "name": "conda-env-tensorflow_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
